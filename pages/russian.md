---
layout: page
title: Стандарт Unicode. UTF-8 и UTF-16.
---

### Содержание

- Цель документа.
- История кодирования. ASCII.
- История создания и принцип работы Unicode.
- Базовые принципы кодирования в Unicode.
- Перевод символа в машинный код. Определение кодировки.
- UTF-8. Алгоритм кодирования.
- UTF-16. Алгоритм кодирования.
- Порядок байт. Маркер порядка байт в Unicode.
- Общая информация о  UTF-32.
- Сравнение UTF-8 и UTF-16. Выводы.


### Цель документа

После прочтения документа читатель должен иметь ясное представление о стандарте кодирования символов Unicode, а также знать, как символы Unicode переводятся в машинный код. Также в данном документе будут изложены принципы кодирования в UTF-8 и UTF-16, коротко UTF-32, представлены основные различия между кодировками, плюсы и минусы каждой.

Документ рассчитан на технических специалистов, имеющих представление о двоичной и шестнадцатеричной системах счисления.

### История кодирования. ASCII.

Кодирование - это представление информационного сообщения в виде кода. Простейшим способом кодирования, известным с древних времен, является сигнальный огонь. 

Первым инструментом, способным передать алфавит и знаки препинания, является азбука Самюэля Морзе. В азбуке появляется термин “кодовая таблица” - это таблица соответствия между символами и их кодами. Развитием азбуки Морзе считается код Бодо. Для передачи каждого символа использовалось 5 импульсов. По факту, код Бодо является первой в мире двоичной системой кодирования информации. Размер каждого символа был фиксирован и составлял 5 бит.

В 1936 г в США была создана кодировочная таблица ASCII (American Standart Code for Information Interchange). ASCII представляет собой таблицу соответствия символов их кодам, размером 8 на 16 ячеек.

Таблица ASCII, за исключением латинских букв и знаков препинания, содержит в себе управляющие символы, такие как начало текста (SOT), перевод строки (LF), конец передачи (EOT), и другие - всего 33 символа. 

На ASCII история развития кодирования могла закончиться, если бы человечество разговаривало только на английском языке, и не использовало символы типа ¿ - “Перевернутый вопросительный знак”. Однако мы знаем, что в мире существуют тысячи языков, и множество специальных символов. Возникла логичная идея - объединить все существующие алфавиты и все спецсимволы в одну таблицу. 

Так появился стандарт Unicode.

### История создания и принцип работы Unicode.

Unicode - стандарт кодирования символов, включающий в себя практически все письменные языки мира. Стандарт был предложен некоммерческой организацией Unicode Consortium в 1991 году. 

Стандарт состоит из двух частей: Universal Character Set, UCS - универсальный набор символов, и Universal Transformation Format, UTF - набор кодировок.

Говоря простым языком, UCS - таблица, где каждому символу соответствует свой код в шестнадцатеричной кодировке, с префиксом U+. 

UTF - это алгоритм перевода шестнадцатеричного кода UCS в двоичный. Иными словами, это перевод промежуточного U+ кода в язык, понятный компьютеру.

### Базовые принципы кодирования в Unicode. 

Символы в Unicode имеют специфическое название - “кодовая точка”. К примеру, латинская буква E будет представлена кодовой точкой U+0045. Стоит отметить, что  строчная буква “e”, а также символы кириллицы “E” и “е” будут иметь собственные коды в таблице - несмотря на одинаковое начертание.

Кодовая точка имеет также дополнительные характеристики, такие как:

- HTML и CSS коды слова.
- Раздел таблицы, в котором находится кодовая точка.
- Название в Unicode (E - Latin Capital Letter E)

Кодовые точки образуют множество, которое называется “кодовое пространство”. Это пространство состоит из 1 114 112 кодовых точек, из которых заняты 128 237 - то есть всего 12%. 

Ниже представлена карта кодового пространства Unicode. Каждое маленькое поле (квадрат) карты содержит 16 * 16 = 256 кодовых точек. В свою очередь, каждое большое поле содержит 65536 кодовых точек. Общее количество больших полей равно 17. Unicode также резервирует “приватные точки” - поля для внутренних нужд приложений.

![Карта расположения кодовых точек]({{ site.url }}/assets/map.png)

Синим цветом в таблице обозначены уже определенные точки, зеленым - приватные кодовые точки. Самое большое пространство - это свободные поля, они отмечены белым.

Первое большое поле (левый верхний квадрат) используется наиболее часто. Оно называется “Базовое мультиязычное поле”, и содержит практически все символы, которые используются в современных текстах. Базовое поле включает в себя латинские буквы, корейский, японский и китайский алфавит, кириллицу, и другие языки. 

Второе поле содержит более специфические языки, например, египетские иероглифы, а также эмодзи. Третье поле содержит китайские символы. 

### Перевод символа в машинный код. Определение кодировки.

Мы выяснили, что каждый символ в Unicode представлен кодовым словом в диапазоне U+0000 - U+10FFFF. Следующая задача - разобраться, как шестнадцатеричный код переводится в двоичный, понятный компьютеру. Для этого преобразования используются форматы кодирования UTF. Их также называют “кодировками”.

Для представления кодовой точки в бинарном виде мы можем использовать 1,2 или 4 байт (8, 16, и 32 бита соответственно). Это и есть упрощенное определение кодировок UTF-8, UTF-16, UTF-32. 

Следует отметить, что в Unicode используется маркер последовательности байт (Byte Order Marker, BOM) - специальный символ, вставляемый в начало текстового потока,  и обозначающий то, что в файле (потоке) используется Юникод, а также для указания кодировки и порядка байтов, с помощью которых символы Юникода были закодированы.

Рассмотрим различные кодировки на примере латинской буквы E:
```
Кодовая точка: U+0045
Шестнадцатеричное число: 45
Десятичное число: 69 (порядковый номер в таблице Unicode)

UTF-8:                             01000101
UTF-16:                   00000000 01000101
UTF-32: 00000000 00000000 00000000 01000101
```

Нетрудно заметить, что для кодировании кодовой точки из Базового поля требуется всего один рабочий байт. В кодировках UTF-16 и UTF-32 все байты, кроме первого, заняты нулями, и не имеют смысла. Для чего же нужны “тяжелые” кодировки, если есть UTF-8? Подробное объяснение этому будет ниже.

### UTF-8. Алгоритм кодирования.

UTF-8 является наиболее распространенной кодировой в веб-пространстве. Эта кодировка использует от 1 до 4 байт для представления символа, и является полностью совместимой с ASCII. UTF-8 широко применяется в UNIX-подобных системах.

Алгоритм кодирования в этой кодировке разделен на несколько этапов.
Сначала нужно выяснить, какое количество байт потребуется для кодирования символа. Для этого используется таблица соответствия:


|Диапазон кодовых точек |Требуемое количество байт|
|-----------------|------------------------|
|00000000-0000007F|1                       |
|00000080-000007FF|2                       |
|00000800-0000FFFF|3                       |
|00010000-0010FFFF|4                       |

\

Как мы уже выяснили выше, для латинской буквы E потребуется всего один байт, т.к. она находится в первом диапазоне. 

Для символа “галочка ✓” потребуется уже 3 байта, т.к. он лежит в третьем диапазоне.

Далее, требуется установить старшие биты первого байта в соответствующее значение:


|Старший бит|Требуемое количество байт|
|-----------|----------------------------|
|0xxxxxxx  |1                            |
|110xxxxx  |2                            |
|1110xxxx  |3                            |
|11110xxx  |4                            |

\

Также нужно определить старшие биты в промежуточных байтах (2-4). Если для кодирования требуется более двух байт, первые два бита в байтах 2-4 всегда принимают значение 10хххххх.



|Количество байт| Значащих бит| Шаблон|
|---------------|-----------------|--------|
|1              |7                |0xxxxxxx|
|2              |11               |110xxxxx 10xxxxxx|
|3              |16               |1110xxxx 10xxxxxx 10xxxxxx|
|4              |21               |11110xxx 10xxxxxx 10xxxxxx 10xxxxxx|

\

Таким образом, мы выяснили, что для символа “галочка” первый байт будет равен 1110хххх. Второй и третий байты будут начинаться с 10хххххх.

Последним шагом в кодировании символа будет установка значащих битов в соответствие с символами Unicode. Начать заполнение нужно с младших битов номера символа, поставив их в младшие биты последнего байта, после чего продолжить справа налево до первого байта. Свободные биты первого байта заполняются нулями.

В итоге, мы получили двоичное представление для символа “галочка”. Это 3 байта:
```
11100010 10011100 10010011
```
Мы выяснили способ представления кодовых точек в кодировке UTF-8. Ниже рассмотрим работу с кодировкой UTF-16.

### UTF-16. Алгоритм кодирования.

UTF-16 представляет собой способ кодирования символов, в котором символы кодируются набором двухбайтовых слов. Диапазон значений от U+0000 до U+FFFF записывается двумя байтами. К примеру, латинская буква E будет записана так:
```
U+0045 	00000000 01000101
```
Обратите внимание, что первый байт полностью забит нулями. 

С помощью двух байт возможно представление только 65535 кодовых точек. Однако, мы знаем, что символов в Unicode значительно больше. Для представления кодовых точек в диапазоне больше U+FFFF применяется уже 4 байта. Кодирование этих байт происходит с помощью “суррогатных пар”. 

Суррогатная пара - это двухбайтовый символ Unicode, который в комбинации с другой суррогатной парой дает символ Unicode в диапазоне выше U+FFFF. Суррогатные пары бывают верхние и нижние (“leading” и “trailing”). 
```
Диапазон верхних суррогатных пар: U+800 - U+DBFF
Диапазон нижних суррогатных пар: U+DC00 - U+DFFF
```
Разберем на примере, как происходит кодирование символов выше U+FFFF с помощью суррогатных пар. Для этого обратимся к символам книги “Канон Перемен”, которая была написана в Китае около 700 года до н.э.

Символ 𝌡 - тетраграмма перемен, будет представлен в UTF-16 в следующем виде:
```
D8 34 DF 21	11011000 00110100 11011111 00100001
```
Здесь первые два байта - это верхняя суррогатная пара U+D834, тогда как вторые два байта U+DF21 являются нижней суррогатной парой.

Сами по себе символы U+D834 и U+DF21 не являются значимыми в Unicode. Иными словами, для этих символов нет буквенного или графического представления. Они зарезервированы конкретно под составление суррогатных пар кодировки UTF-16, и “работают” только вместе.

Рассмотрим также символ “галочка” из раздела о кодировке UTF-8. В UTF-16 этот символ будет представлен так:
```
U+2713 	00100111 00010011
```
Обратите внимание, что символ “галочка” в кодировке UTF-16 занимает только 2 байта, тогда как в UTF-8 для его кодирования потребовалось 3 байта.

В кодировке UTF-16 порядок байт может быть разным. Этот порядок зависит от архитектуры процессора, на котором производится обработка данных. Символ перемен 𝌡 может быть представлен в двух вариантах:
```
D8 34 DF 21	11011000 00110100 11011111 00100001
34 D8 21 DF	00110100 11011000 00100001 11011111
```
Первый вариант называется Big Endian (BE), второй - Little Endian (LE). О том, что означают эти форматы, и как процессор различает их, будет рассказано в следующей главе.


### Порядок байт. Маркер порядка байт в Unicode.

Различные типы процессоров используют разный порядок байт.

Big Endian является порядком байт “от старшего к младшему”. Он соответствует привычному порядку записи арабских цифр - слева направо.Такой порядок байт используется в процессорах SPARC, Motorola, IBM, а также в протоколе TCP/IP.

Порядок Little Endian, в свою очередь  - ”от младшего к старшему”. К примеру, число 123 в этом порядке было бы записано как 321. Этот порядок байт применяется в процессорах семейства x86, а также в интерфейсах USB и PCI.

Возникает вопрос - как процессор определяет, какой порядок байт использовать для работы с информационным блоком? Для этого в Unicode был введен специальный символ - U+FEFF. Этот символ уже известен нам как “маркер последовательности байт” (Byte Order Marker, BOM). 
```
BOM для кодировки UTF-16BE: 0xFE 0xFF
BOM для кодировки UTF-16LE:  0xFF 0xFE
```
Стоит отметить, что в Unicode не существует символа U+FFFE. Это сделано для однозначного определения BOM, и следовательно, порядка байт. 

Кодировка UTF-8 не использует BOM для определения порядка байт. При этом стандарт подразумевает добавление BOM в начало файла, закодированного в UTF-8. Это нужно для однозначного определения факта, что файл имеет кодировку UTF-8.

### Общая информация о  UTF-32.

Стандарт Unicode также может быть кодирован в UTF-32. Эта кодировка всегда использует 4 байта для представления любого символа Unicode. Иными словами, даже символ  “E“ из Базового поля в UTF-32 будет иметь вид 00000000 00000000 00000000 01000101. 

Как видно, в примере первые три байта - “лишние”. Отсюда главный минус кодировки - слишком большой объем, который займет текст на диске компьютера. Особенно это заметно при работе с символами из Базового поля.

Кодировка UTF-32 также должна иметь BOM в начале текста, и может быть как Big Endian, так и Little Endian.

### Сравнение UTF-8 и UTF-16. Выводы.

Теперь мы знаем особенности кодировок UTF-8 и UTF-16, и можем сделать следующие наблюдения и выводы:

- UTF-8 идеально подойдет для работы с латиницей и управляющими символами диапазона ASCII, ведь для кодирования этих символов потребуется всего 1 байт. Если ваш продукт содержит латиницу (даже вперемешку с другими языками), UTF-8 будет отличным выбором.

- В случае, если ваш продукт будет работать с кириллицей, греческим языком и ивритом, обе кодировки будут использовать по 2 байта для представления кодовой точки. Если же вы планируете в основном работать с азиатскими языками, выбор UTF-16 будет предпочтительней, т.к. для записи символа потребуется 2 байт (вместо трех в UTF-8).

- Если ваш продукт рассчитан на работу с веб-пространстве, или в Unix-системах, выбор UTF-8 будет предпочтительней. В свою очередь, Windows, Java, Python, C# используют UTF-16 в качестве внутренней кодировки.

- UTF-8 не зависит от порядка байт. UTF-16 может иметь два варианта порядка байт, и обязательно требует использование BOM. В UTF-8 BOM зачастую вовсе не используют.

- Общение между компьютерами происходит в основном в диапазоне ASCII, и здесь UTF-8 имеет полное преимущество.

- UTF-16 лучше подойдет для представления данных в памяти, т.к. порядок байт не будет иметь значения, а индексация по кодовым точкам будет выполняться быстрее. 

- Кодовая точка в UTF-8 может содержать от 1 до 4 байт, что делает затруднительным манипуляции со строкой (например, вычисление количества символов в строке).

- UTF-8 является самосинхронизирующейся кодировкой. Это означает, что при повреждении какого-либо байта в строке будет неверным только один символ, остальная часть строки будет представлена корректно.


